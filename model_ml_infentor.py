# -*- coding: utf-8 -*-
"""Model_ML_InFenTor(untuk download).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dw9ap_qZMtZ4K_z1RE5VevoZ_DFXkJQb
"""

from google.colab import drive
drive.mount('/gdrive')

"""# Import Dataset"""

import pandas as pd


data = pd.read_excel('/gdrive/MyDrive/InFenTor/DATASET_BETA.xlsx')
data.dropna(inplace=True)
data.head()

data['karir'] = data['karir'].replace('Desainer', 'Designer')
df = data.copy()

df.karir.value_counts()

jumlah_col = 2000

df1 = df[df['karir'] == "IT"]
df1 = df1.sample(frac=1, random_state=42)
df1 = df1.head(jumlah_col)

df2 = df[df['karir'] == "Kesehatan"]
df2 = df2.sample(frac=1, random_state=42)
df2 = df2.head(jumlah_col)

df3 = df[df['karir'] == "Akuntan"]
df3 = df3.sample(frac=1, random_state=42)
df3 = df3.head(jumlah_col)

df4 = df[df['karir'] == "Guru"]
df4 = df4.sample(frac=1, random_state=42)
df4 = df4.head(jumlah_col)

df5 = df[df['karir'] == "Designer"]
df5 = df5.sample(frac=1, random_state=42)
df5 = df5.head(jumlah_col)

df_merge = pd.concat([df1, df2, df3, df4 , df5])
df_merge.info()

import re
import string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

import nltk
nltk.download('stopwords')
nltk.download('wordnet')

import re
def preprocess_text(text):
    # Mengubah teks menjadi huruf kecil
    text = text.lower()

    # Menghapus tanda baca dan karakter selain alfabet
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    return text

df_merge['text'] = df_merge['text'].apply(preprocess_text)

df_merge = df_merge.sample(frac=1, random_state=42)

df_merge.head()

"""# Modelling"""

from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dropout, GRU, Activation, Embedding, Bidirectional,SpatialDropout1D, BatchNormalization, Conv1D, MaxPool1D

data = df_merge.copy()

MAX_NB_WORDS = 5000
# Max number of words in each description.
MAX_SEQUENCE_LENGTH = 250
# This is fixed.

EMBEDDING_DIM = 100

# Tokenizer
tokenizer = Tokenizer(num_words=MAX_NB_WORDS,
                      filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~',
                      lower=True, oov_token='OOV')

# Fit Tokenizer
tokenizer.fit_on_texts(data['text'].values)

# Jadikan menjadi Sequences
X = tokenizer.texts_to_sequences(data['text'].values)

word_index = tokenizer.word_index

import json

# Menyimpan word_index sebagai file JSON
with open('/gdrive/MyDrive/InFenTor/tokenizer_word_index_try_pertama.json', 'w') as f:
    json.dump(word_index, f)

word_index = tokenizer.word_index
word_index

# agar sama panjang

X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)

tokenizer.texts_to_sequences(["saya suka menggambar gedung"])

# Melihat tokenizer kita
# tokenizer.word_index

replace_category = {
'Akuntan':0, 'Designer':1, 'Guru':2, 'Kesehatan':3, 'IT':4
}
Y = pd.get_dummies(data.replace({"karir": replace_category})['karir'].values)
print('Shape of label tensor:', Y.shape)

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, 300, input_length=X.shape[1]))
model.add(LSTM(256, dropout=0.5, recurrent_dropout=0.5))

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(5, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs = 10
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)

model.summary()

import matplotlib.pyplot as plt

accr = model.evaluate(X_test,Y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show();



# Simpan model Keras dalam format SavedModel
tf.saved_model.save(model, '/gdrive/MyDrive/InFenTor/saved_model')

# Simpan model ke file HDF5
model.save('/gdrive/MyDrive/InFenTor/model_kedua_embed10k.h5')

